{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from astropy import constants as const\n",
    "import h5py\n",
    "\n",
    "#these modules shuould all be in your current directory, as well as the line_lists directory\n",
    "from atmospheric_model import semianalytical, xs_DACE_multi\n",
    "import preprocessing\n",
    "from mini_inferno import apply_uniform_logPrior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae3a0e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial is going to walk through a simple retrieval performed on a simulated dataset. The main emphasis of the tutorial is to get you thinking about the various steps required to match the model to the data, with a particular focus on accounting for PCA or SysRem preprocessing of the data. DISCLAIMER: There are several different ways to achieve this, and this is not intended to be a difinitive version of data preprocessing and model filtering (in fact, I threw this together quite quickly and wouldn't be surprised if there're biases in the methodology here!).\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The data are a simulated transit dataset based on real CRIRES+ spectra. The object is a hot Jupiter with H$_2$O and CO injected into the transmission signal. The specific planet parameters used for the injection are also included with the data. All wavelengths are in Angstrom.\n",
    "\n",
    "First let's load in the data and check the format. You can either use the `.npz` or `.h5` format to load. The dataset contains the 3D datacubes of the spectra and noise (S and Se, respectively) ordered as [order $\\times$ time $\\times$ wl]. The wavelengths are stored in W [order $\\times$ wl], and the time [BJD] is stored in time along with the period and central transit time.\n",
    "\n",
    "Let's first load up the data and make some basic plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in .npz file as a dictionary\n",
    "d = dict(np.load('simulated_data.npz'))\n",
    "# or load in .h5 file as dictionary\n",
    "#with h5py.File('simulated_data.h5','r') as F: d = {k:F[k][()] for k in F}\n",
    "\n",
    "#check the contents of the data dictionary\n",
    "print(d.keys())\n",
    "\n",
    "# compute the phase\n",
    "ph = (d['bjd'] - d['T0']) / d['period'] - 1131 # subtract epoch\n",
    "W,S,Se = d['W'],d['S'],d['Se']\n",
    "\n",
    "#compute transit weighting from the transit model\n",
    "tmodel = d['transit']\n",
    "tweight = (1. - tmodel) / np.ptp(tmodel)\n",
    "\n",
    "#get the injected model parameters\n",
    "p_inject = d['p_inject']\n",
    "print(p_inject)\n",
    "Temp,Pcloud,Xray,dv,Xh2o,Xco = p_inject # store the injected model parameters for later\n",
    "Kp=200\n",
    "v_sys=0\n",
    "\n",
    "#print out some details of the datasets\n",
    "print(\"S.shape =\", S.shape)\n",
    "print(\"Se.shape =\", Se.shape)\n",
    "print(\"W.shape =\", W.shape)\n",
    "print(\"ph.shape =\", ph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 10 # define test order\n",
    "\n",
    "# plot one of the orders\n",
    "ax = plt.subplot(xlabel=r'wavelength [$\\AA$]',ylabel='phase')\n",
    "ax.pcolormesh(W[order],ph,S[order])\n",
    "\n",
    "#plot the spectra + uncertainties\n",
    "fig, ax = plt.subplots(2,sharex=1)\n",
    "ax[0].set(xlabel=r'wavelength [$\\AA$]',ylabel='flux')\n",
    "ax[1].set(xlabel=r'wavelength [$\\AA$]',ylabel='error')\n",
    "for w,spec in zip(W,S[:,10]): ax[0].plot(w,spec,lw=1,color='k')\n",
    "for w,spec_err in zip(W,Se[:,10]): ax[1].plot(w,spec_err,lw=0.6,color='0.5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83793dcb",
   "metadata": {},
   "source": [
    "### The Atmospheric Model\n",
    "\n",
    "We're going to use a simple semi-analytic model of Heng & Kitzmann (2017) that I also used in Gibson et al. (2020) in a slightly modified form. This is:\n",
    "$$\n",
    "R(\\lambda)=R_{0}+H\\left[\\gamma+\\ln \\left(\\frac{P_{0}}{\\bar mg} \\sqrt{\\frac{2 \\pi R_{0}}{H}}\\right)\\right] + H \\ln\\sum_j \\chi_j \\sigma_j(\\lambda),\n",
    "$$\n",
    "where $R_0$, $P_0$ are the reference radius and pressure, $H = \\frac{kT}{\\bar mg}$ is the atmopsheric scale height, $g$ is the surface gravity, $\\chi_j$ is the abundance of species $j$, and $\\sigma_j$ is the atmospheric cross-section for species $j$, which is interpolated to a temperature $T$.\n",
    "\n",
    "I have also included a braodening parameter, $\\Delta V$, which is the standard deviation of a Gaussian broadening kernel. Note that in order for this to make sense, the wavelength has to be sampled in a very specific way (i.e. logarithmically with $d\\ln\\lambda = {\\rm const}$) in order to have a constant resolution (and therefore constant velocity spacing). It is useful to remember that the resolution of a spectrograph also changes with wavelength, and therefore it is extremely tricky to apply an instrumental profile to our model spectrum. (But that is not important here, as our simulated spectrum is only broadened using the Gaussian kernel.)\n",
    "\n",
    "The full parameter vector for the 1D model is: [$T$,$P_{\\rm cloud}$,$\\chi_{\\rm ray}$,$\\Delta V$,$\\chi_{\\rm H_2O}$,$\\chi_{\\rm CO}$]\n",
    "\n",
    "The model assumes an isothermal atmosphere and isobaric (at least for the cross-sections). This is a very fast and simple model that is very useful to play around with to test the retreival framework. However, there are some important limitations in addition to the isothermal assumption. For example, as the cross-sections have no pressure sensitivity, and the mean molecular weight is fixed and not linked to the abundance (this could be modified of course), the cloud deck pressure and abundances are completely degenerate. In other words you can increase both the abundance of water and CO by a factor of 10, and raise the cloud deck (or corresponding scattering parameter), and you will get an identical atmosphere (plus vertical offset). This leads to strong degeneracies in the abundances, and it is probably easier to just fix one of the abundances to begin with. The relative abundances on the other hand should be well constrained.\n",
    "\n",
    "There are quite a few differnet modes for the model:\n",
    "- tspec_r: (effective) planet radius\n",
    "- tspec_rprs: planet-to-star radius ratio\n",
    "- tspec_nf: negative flux, i.e. $1-(R_{\\rm p}/R_\\star)^2$\n",
    "- tspec_ndf: negative 'differential' flux, this is after subtracting the continuum.\n",
    "\n",
    "It is important to consider which one to use for differnet approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06740098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D atmosphere pars\n",
    "Rstar = 1.458*const.R_sun.value #stellar radius in m\n",
    "Mpl = 1.184*const.M_jup.value #planet mass in kg\n",
    "Rpl = 0.118*Rstar #stellar radius in m\n",
    "g = const.G.value * Mpl / Rpl**2 #surface gravity m/s^2\n",
    "mmw = 2.33 #mean molecular weight\n",
    "P0 = 1.e-2 #reference pressure in bars\n",
    "\n",
    "#load in the cross-sections using provided function\n",
    "wl,temp_grid,pressure_grid,xs = xs_DACE_multi('H2O CO'.split(),wl=d['wl_model'],units='m2')\n",
    "\n",
    "#strip pressure dependence from cross-sections to simplify\n",
    "xs[0] = xs[0].squeeze()\n",
    "xs[1] = xs[1].squeeze()\n",
    "\n",
    "#define semi-analytical model\n",
    "model = semianalytical(wl,Rpl,P0,g,mmw,Rstar,temp_grid,xs)\n",
    "\n",
    "#important to note that pars are not in log space here\n",
    "#test the model, note pars = [temp,Pcloud,mix_h2,vel_broad,mixing_ratios x Nspecies]\n",
    "r = model(np.array([2300,1e-2,1] + [2] + [1e-5,1e-5]))\n",
    "\n",
    "sigma = 3\n",
    "template = model.tspec_ndf(p_inject)\n",
    "template = model.tspec_ndf(np.array([2300,1e-2,1] + [sigma] + [1e-5,1e-5]))\n",
    "template_h2o = model.tspec_ndf(np.array([2300,1e-2,1] + [sigma] + [1e-5,1e-100]))\n",
    "template_co = model.tspec_ndf(np.array([2300,1e-2,1] + [sigma] + [1e-100,1e-5]))\n",
    "\n",
    "fig,ax = plt.subplots(2)\n",
    "ax[0].plot(wl,model(np.array([2300,1e-2,10000] + [2] + [1e-5,1e-5])),'-')\n",
    "ax[1].plot(wl,template,'-')\n",
    "ax[0].set_ylabel(r'$R_{\\rm p}$ [m]')\n",
    "ax[1].set_ylabel(r'$\\Delta F$')\n",
    "_ = ax[1].set_xlabel(r'wavelength [$\\AA$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53c2c5-480e-4554-ba8d-e4437ce959e3",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "We're going to keep things simple and use a weighted PCA (which accounts for noise along columns, i.e. wavelength dependence). But we will use a more generic filtering process so you can easily use SysRem instead if you prefer.\n",
    "\n",
    "Generally, the first step is to *divide* the spectra through by the stellar + telluric model, which is cricital to preserving the line depths and remaining consistent with our model. Often, we do this by first dividing through by the average spectrum, then apply PCA / SysRem. But an alternative is to use PCA / SysRem to model the stellar + telluric spectra directly. In this case, it is crucial to divide through by the full model to preserve line depths. If we first divide by the average spectrum, it is ok to then subtract a PCA / SysRem model - even if the systematics fit by the model are multiplicative, if they are small then this is a good approximation.\n",
    "\n",
    "We are going to use 5 PCA components. The simulations actually have no systematics injected, but we should still be able to filter our data and get the consistent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we do a mean division and scale the noise appropriately\n",
    "S_ave = np.mean(S,axis=-2)[...,None,:]\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "  X = S / np.abs(S_ave)\n",
    "  Xe = Se / np.abs(S_ave)\n",
    "#clean up any bad datapoints (from zero division in the normalisation)\n",
    "Xe[np.isnan(X)] = np.inf\n",
    "X[np.isnan(X)] = 0.\n",
    "\n",
    "# perform sysrem and subtract model from X\n",
    "N_components = 5 # let's remove 5 components. How do we get this number in general!? A thorny model selection problem...\n",
    "# run PCA -> this version will do a mean subtraction and column normalisation first, which weights the columns when exactracting basis U\n",
    "U,Weights,M = preprocessing.pca(X,N=N_components,mean_sub=True,add_bias=True,norm_cols=True) # run PCA\n",
    "\n",
    "# we can double check that we get exact same model when refitting...\n",
    "# The full linear fit is U (U^T U)^{-1} U.T Y, but (U^T U)^{-1} is the identity matrix for orthogonal basis functions\n",
    "M_refit = np.einsum('...aj,...tj,...tl->...al',U,U,X,optimize=True)\n",
    "print(\"fitting ok?\",np.all(np.isclose(M,M_refit))) # show that re-fitting is identical\n",
    "\n",
    "# should we refit using the time-dependent noise??\n",
    "# to do this we need to use the uncertainties as this 'breaks' the 'orthogonality' of the basis vectors\n",
    "inv_stdev = 1./np.nanmean(np.where(np.isfinite(Xe),Xe,np.nan),axis=-1)\n",
    "psinv_scaled = np.linalg.pinv(U * inv_stdev[:,:,np.newaxis]) * inv_stdev[:,np.newaxis,:]\n",
    "M = np.einsum('...aj,...jt,...tl->...al',U,psinv_scaled,X,optimize=True) # I've changed the order of the 2nd term as U^dagger is transposed\n",
    "print(\"fitting with uncertainties should be different so=false:\",np.all(np.isclose(M,M_refit))) # show that this is different?\n",
    "\n",
    "# we can also double check if the basis vectors are orthogonal\n",
    "# this is guaranteed from PCA (but we also added a bias term, it should still be orthogonal if we've subtracted a mean)\n",
    "print(\"are the basis funcitons orthogonal basis?\",np.all(np.isclose( U[0].T @ U[0], np.eye(U[0,0].size))))\n",
    "\n",
    "# we then want to get the residuals from the model - M is rescaled back according to mean sub / normalisation above\n",
    "R = X - M # residuals\n",
    "Re = Xe # if subtracting the model then uncertainties are the same\n",
    "\n",
    "#check for any bad data in our arrays\n",
    "print(np.isnan(R).sum())\n",
    "print(np.isnan(Re).sum())\n",
    "print(np.isinf(R).sum())\n",
    "print(np.isinf(Re).sum()) # infs are fine in the uncertainties as it just removes them from the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dd320-ef5d-4881-ab0e-f4a1d09da840",
   "metadata": {},
   "source": [
    "### Cross-correlation + Velocity Sum\n",
    "\n",
    "This was covered in yesterday's tutorial. Let's just run a simple version to check what's in the data! We'll define a simple cross-correlation and a simple velocity summation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c053c-22db-494c-a0f8-cb9dc5f21088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define velocity and Kp for our search\n",
    "v = np.linspace(-200,200,1000) #define range to perform cc, in km/s\n",
    "K = np.linspace(-300,300,600)\n",
    "\n",
    "def cross_correlate(v,W,R,Re,wl,template):\n",
    "    \"\"\"\n",
    "    Simple cross-correlation function, looping over every order.\n",
    "    This uses simple linear interpolation - this isn't always appropriate !!\n",
    "    \"\"\"\n",
    "    \n",
    "    CC = np.empty((R.shape[0],R.shape[1],v.size)) #create empty array for CC function\n",
    "    for order in range(CC.shape[0]): #loop over orders\n",
    "    \n",
    "        W_shifted = np.outer((1.-v/299792.458),W[order]) #create array of shifted wavelengths for each v_sys\n",
    "        shifted_templates = np.interp(W_shifted,wl,template) #interpolate the template model to every v_sys\n",
    "    \n",
    "        #and compute the cross correlation as before\n",
    "        CC[order] = np.dot(R[order]/Re[order]**2,shifted_templates.T)\n",
    "\n",
    "    return CC\n",
    "\n",
    "plt.subplots(1)\n",
    "CC = cross_correlate(v,W,R,Re,wl,template).sum(axis=0)\n",
    "plt.pcolormesh(v,ph,CC)\n",
    "plt.xlabel('v [km/s]')\n",
    "_ = plt.ylabel('phase')\n",
    "#plt.pcolormesh(v,ph,CC-np.median(CC,axis=-1)[...,None]) # sometimes want to subtract average depending on normlisation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b688c02-ab27-4372-8b73-cd57703df20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vel_sum(ph,v,CC,K,tweight=None,norm=True,K_lims=[-250,-150],v_lims=[-100,100]):\n",
    "  \"\"\"\n",
    "  Simple function to do a velocity summation over planet's velocity\n",
    "  \"\"\"\n",
    "  if tweight is None: #by default weight everything equally\n",
    "    tweight = np.ones(CC.shape[0])   \n",
    "  \n",
    "  #define arrays for storage\n",
    "  shifted = np.empty(CC.shape)\n",
    "  CC_map = np.empty((K.size,v.size))\n",
    "\n",
    "  #loop over the Ks\n",
    "  for i in range(K.size):\n",
    "    vp = K[i]*np.sin(ph*2.*np.pi) #+25 #compute vp for each K\n",
    "    for j in range(vp.size): #loop over velocities and interpolate to planet's rest frame assuming given Kp\n",
    "      shifted[j] = np.interp(v+vp[j],v,CC[j])\n",
    "\n",
    "    #store weighted sum for each Kp\n",
    "    CC_map[i] = np.sum(shifted * tweight[:,None], axis=0)\n",
    "  \n",
    "  if norm:\n",
    "    CC_map_noise = CC_map[(K>K_lims[0])*(K<K_lims[1]),:][:,(v>v_lims[0])*(v<v_lims[1])]\n",
    "    CC_map = (CC_map - CC_map_noise.mean()) / CC_map_noise.std() # rescale map to get noise\n",
    "    \n",
    "  return CC_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f090acd-45c9-457f-9954-234740420d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's sum over the CC function to get Kp-vys map\n",
    "CC_map = vel_sum(ph,v,CC,K,tweight=tweight,norm=1) #it doesn't make any difference whether we subtract the mean\n",
    "\n",
    "#we'll also repeat for the CO and H2O models\n",
    "CC_h2o = cross_correlate(v,W,R,Re,wl,template_h2o).sum(axis=0)\n",
    "CC_co = cross_correlate(v,W,R,Re,wl,template_co).sum(axis=0)\n",
    "CC_map_h2o = vel_sum(ph,v,CC_h2o,K,tweight=tweight,norm=1) #it doesn't make any difference whether we subtract the mean\n",
    "CC_map_co = vel_sum(ph,v,CC_co,K,tweight=tweight,norm=1) #it doesn't make any difference whether we subtract the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf278d2-2041-4738-a8cb-b851c77b8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(3,sharex=True)\n",
    "q = ax[0].pcolormesh(v,K,CC_map)\n",
    "plt.colorbar(q)\n",
    "ax[0].text(-190,200,'full model',color='w')\n",
    "q = ax[1].pcolormesh(v,K,CC_map_h2o)\n",
    "plt.colorbar(q)\n",
    "q = ax[2].pcolormesh(v,K,CC_map_co)\n",
    "plt.colorbar(q)\n",
    "plt.xlabel('vel [km/s]')\n",
    "ax[1].set_ylabel(r'$K_{\\rm p}$ [km/s]')\n",
    "\n",
    "ax[0].text(-190,200,'full model',color='w')\n",
    "ax[1].text(-190,200,'H$_2$0',color='w')\n",
    "_ = ax[2].text(-190,200,'CO',color='w')\n",
    "\n",
    "#let's check where the peak signal is found (hopefuly close to the injected values!!)\n",
    "kp_max,v_max = np.where(CC_map==CC_map.max())\n",
    "print(\"signal @ K={}, v={}\".format(K[kp_max],v[v_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79266b-03d7-426b-a27a-e27e0d2dab37",
   "metadata": {},
   "source": [
    "### The 3D forward model\n",
    "\n",
    "Now we've explored some basics of the dataset, let's start to build up a 3D forward model and compute the likelhood function.\n",
    "\n",
    "First let's define a basic function to compute the 3D forward model. This takes in a 1D model, addionally Kp and v_sys parameters, then interpolates the 1D model into a 3D array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2aa45-1f76-4f8a-8aeb-cd99d4f49cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3D(pars,phase,W,wl_model,model1D,transit_weight,bvc=0,out=None,temp_array=None):\n",
    "  \"\"\"\n",
    "  pars - parameter vector for our forward model\n",
    "  phase - orbital phase\n",
    "  W - 2D wavelength array (ie for each order) [all wavelengths are in Angstrom]\n",
    "  model1D - callable 1D model, assuming that it is normalised to 0 (ie continuum of planet spec is at 0)\n",
    "  transit_weight - weighting vector to apply transit model weighting\n",
    "  bvc - barycentric velocity correction can be added if required and/or correction for v_sys\n",
    "  out - array to store model (not required, but predefining memory can sometimes speed things up a lot!)\n",
    "  temp_array - other array for outputs\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  #compute planet velocity\n",
    "  vp = pars[7]*np.sin(phase*2.*np.pi) + pars[6] + bvc\n",
    "\n",
    "  #pre-compute (1-vp/c)\n",
    "  one_minus_v_c = 1. - vp*3.3356409519815205e-06\n",
    "  \n",
    "  #call the 1D model with first 6 parameters\n",
    "  template = model1D(pars[:6])\n",
    "\n",
    "  #define the output array - this can be provided to (potentially) speed things up\n",
    "  if out is None: out = np.empty([W.shape[0],phase.size,W.shape[1]])\n",
    "  \n",
    "  #loop over orders and interpolate 1D model to 2D grid for each\n",
    "  for ord in range(W.shape[0]):\n",
    "    \n",
    "    #get new wavelength array to interpolate to\n",
    "    W_shifted = np.outer(one_minus_v_c,W[ord],out=temp_array)\n",
    "    \n",
    "    #pass only subset of 1D model - this can speed things up but haven't checked recently\n",
    "    low,up = np.searchsorted(wl_model,[W_shifted.min(),W_shifted.max()])\n",
    "    \n",
    "    #interpolate to output\n",
    "    out[ord] = np.interp(W_shifted,wl_model[low:up+1],template[low:up+1]) * transit_weight[:,np.newaxis] #signal relative to zero continuum\n",
    "        \n",
    "  return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834aa826-7267-4a46-aef2-2ff32a5aa7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the forward model works ok\n",
    "p = np.array([Temp,Pcloud,Xray,dv,Xh2o,Xco] + [v_sys,Kp])\n",
    "\n",
    "# for this version, we should use the differential flux to subtract the continuum\n",
    "args = [ph,W,wl,model.tspec_ndf,tweight]\n",
    "FM = model3D(p,*args)\n",
    "\n",
    "#plot up the model for a specific order plus a few slices\n",
    "fig,ax = plt.subplots(2,sharex=True)\n",
    "ax[0].pcolormesh(W[order],ph,FM[order])\n",
    "ax[1].plot(W[order],FM[10,40])\n",
    "ax[1].plot(W[order],FM[10,50])\n",
    "ax[1].set_xlabel(r'wavelength [$\\AA$]')\n",
    "ax[0].set_ylabel(r'phase')\n",
    "ax[1].set_ylabel(r'flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5f54c-8653-4c36-ba08-322f274e8917",
   "metadata": {},
   "source": [
    "### The likelihood function\n",
    "\n",
    "Now that we have a forward model, we can define our likelihood.\n",
    "\n",
    "We also want to consider reparameterising some of our inputs, as we'd like to fit for the log of the cloud deck pressure, and the log of the abundances. We'll handle that within the likelhood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fa190-abfe-43dc-be83-559a68a6bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(pars,model_3D,phase,W,R,Re,wl_model,model1D,transit_weight,*args,bvc=0,out=None,temp_array=None):\n",
    "  \"\"\"\n",
    "  Simple function to compute the log Likelihood. We'll add in a generic *args so we can add more inputs later.\n",
    "\n",
    "  There are two extra pars to add - alpha and beta\n",
    "  \n",
    "  \"\"\"\n",
    "  p = np.copy(pars) # copy the vector\n",
    "  p[1:3] = 10**p[1:3] # Pcloud + X_ray\n",
    "  p[4:6] = 10**p[4:6] # abundances\n",
    "  \n",
    "  #compute 3D model\n",
    "  M3D = model_3D(p[:-2],phase,W,wl_model,model1D,transit_weight,*args,bvc=bvc,out=out,temp_array=temp_array)\n",
    "\n",
    "  #make a transit filter - should we only compute the likelihood for points within the transit!? I'm not particularly sure anymore...\n",
    "  tfilter = transit_weight > 0\n",
    "\n",
    "  #compute the chi2\n",
    "  chi2 = np.square((R[:,tfilter,:] - p[-2]*M3D[:,tfilter,:]) / Re[:,tfilter,:]).sum() / p[-1]**2\n",
    "  \n",
    "  #get the effective number of parameters (we should ignore any infs in the noise as they don't contribute to beta!)  \n",
    "  Neff = np.isfinite(Re[:,tfilter,:]).sum() # move outside likelihood?\n",
    "  \n",
    "  return -0.5 * chi2 - Neff * np.log(p[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3c5ec-54d0-460f-8774-a69362b56de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the likelihood works ok - remember to add alpha and beta to the vector\n",
    "# also remember we are passing the log10 of some parameters now...\n",
    "p = np.array([Temp,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [1,1])\n",
    "\n",
    "#predefine the arguments list\n",
    "args = [ph,W,R,Re,wl,model.tspec_ndf,tweight]\n",
    "print(\"logL =\", log_likelihood(p,model3D,*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be469c-8cf5-4548-8a1a-e857b51e796e",
   "metadata": {},
   "source": [
    "### Defining a prior and the posterior distribution\n",
    "\n",
    "Our final step before being ready to consider a retrieval is to define our prior and our posterior distribution. We're just going to use a simple (improper) uniform prior, where we simply set lower and upper bounds for each of our parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8e29b-49f8-4df0-b584-dd5d59eda9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's just define lower and upper limits as a list of tuples\n",
    "bounds = [(1000,4000),(-3,2),(-10,5),(1,10),(-10,-2),(-10,-2),(-50,50),(150,250),(0.1,2),(0.5,2)]\n",
    "\n",
    "#while we're at it let's define a list of labels for our parameters (useful for plotting)\n",
    "labels = [r'$T$',r'$P_{\\rm cloud}$',r'$\\chi_{\\rm Ray}$',r'$\\Delta{V}$',r'$\\chi_{\\rm H_2O}$',r'$\\chi_{\\rm CO}$',r'$v_{\\rm sys}$',r'$K_{\\rm p}$',r'$\\alpha$',r'$\\beta$']\n",
    "\n",
    "#we can just use a little wrapper function from mini_inferno\n",
    "logP = apply_uniform_logPrior(log_likelihood,bounds) #apply prior to logL using transit uniform bounds\n",
    "\n",
    "#let's test it - should just give us the same value unless we step out of the prior bounds, where it returns -np.inf\n",
    "print(\"logP =\", logP(p,model3D,*args))\n",
    "print(\"logP =\", logP(np.array([800,-2,0] + [5.021643907242071] + [-5,-5] + [0,200] + [1,1]),model3D,*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc89c31-37f0-462d-953d-b73c5d1a442d",
   "metadata": {},
   "source": [
    "Instead of a retreival, let's just take a slice through our posterior for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d1734-1f0b-43c8-9393-d00eb25e0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define a range of temperatures and keep everything else fixed\n",
    "temps = np.linspace(900,3500,100) # define a range of temperatures\n",
    "\n",
    "logL_slice = [log_likelihood(np.array([q,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [1,1]),model3D,*args) for q in temps]\n",
    "logP_slice = [logP(np.array([q,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [1,1]),model3D,*args) for q in temps]\n",
    "\n",
    "#then lets plot them\n",
    "fig,ax = plt.subplots(2)\n",
    "ax[0].plot(temps,logL_slice,'k-')\n",
    "ax[0].plot(temps,logP_slice,'r-')\n",
    "ax[1].plot(temps,np.exp(logL_slice-np.max(logL_slice)))\n",
    "ax[1].axvline(Temp,ls=':',color='darkgreen') # plot actual value\n",
    "ax[0].axvline(Temp,ls=':',color='darkgreen') # plot actual value\n",
    "ax[0].set_ylabel(r'$\\ln p(T) + C$')\n",
    "ax[1].set_ylabel(r'$\\propto p(T)$')\n",
    "ax[1].set_xlabel(r'$T$')\n",
    "ax[0].axvline(temp_grid[0][-1],ls='--',color='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965b44e-e732-4217-a561-c61f5ad7a27e",
   "metadata": {},
   "source": [
    "Hmm, we're way off! The maximum likelihood / posterior is way smaller than it ought to be! What is going wrong? Let's try another parameter.\n",
    "\n",
    "(The weird kink at $\\sim2,900$K is related to the maximum temperature of the cross-section available. So the cross-section is no longer changing, just the scale height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285de6b5-e7cb-4130-b2e1-2361bc67ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define a range of temperatures and keep everything else fixed\n",
    "alpha_slice = np.linspace(0.1,2,100) # define a range of temperatures\n",
    "logL_slice = [log_likelihood(np.array([Temp,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [q,1]),model3D,*args) for q in alpha_slice]\n",
    "logP_slice = [logP(np.array([Temp,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [q,1]),model3D,*args) for q in alpha_slice]\n",
    "\n",
    "#then lets plot them\n",
    "fig,ax = plt.subplots(2)\n",
    "ax[0].plot(alpha_slice,logL_slice,'k-')\n",
    "ax[0].plot(alpha_slice,logP_slice,'r-')\n",
    "ax[1].plot(alpha_slice,np.exp(logL_slice-np.max(logL_slice)))\n",
    "ax[1].axvline(1) # plot actual value\n",
    "ax[0].set_ylabel(r'$\\ln p(T) + C$')\n",
    "ax[1].set_ylabel(r'$\\propto p(\\alpha)$')\n",
    "ax[1].set_xlabel(r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac04020-ccc2-4212-9eca-0071ca0554c9",
   "metadata": {},
   "source": [
    "Again, we're way off! Let's think again about our forward model.\n",
    "\n",
    "One thing we do with the data is that we normalise it by dividing through by the mean of the spectra (or we might do some more complex blaze correction). But ultimately we normalise the data so it's all roughly on the same level. As the exoplanet's signal is also there, we must do the same thing with our forward model. So the first correction we'll make is just to divide each spectrum (more specifically each spectral order) by the mean along the wavelength direction. To do this we'll write a new foward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1e81f-f897-4d7b-81cb-490e5a7ed257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3D_with_norm(pars,phase,W,wl_model,model1D,transit_weight,bvc=0,out=None,temp_array=None):\n",
    "  \"\"\"\n",
    "  same as before - now with some normalisation...  \n",
    "  \"\"\"\n",
    "  \n",
    "  #compute planet velocity\n",
    "  vp = pars[7]*np.sin(phase*2.*np.pi) + pars[6] + bvc\n",
    "\n",
    "  #pre-compute (1-vp/c)\n",
    "  one_minus_v_c = 1. - vp*3.3356409519815205e-06\n",
    "  \n",
    "  #call the 1D model with first 6 parameters\n",
    "  template = model1D(pars[:6])\n",
    "  \n",
    "  if out is None: out = np.empty([W.shape[0],phase.size,W.shape[1]])\n",
    "  \n",
    "  #loop over orders and interpolate 1D model to 2D grid for each\n",
    "  for ord in range(W.shape[0]):\n",
    "    \n",
    "    #get new wavelength array to interpolate to\n",
    "    W_shifted = np.outer(one_minus_v_c,W[ord],out=temp_array)\n",
    "    \n",
    "    #pass only subset of 1D model - this can speed things up a lot!\n",
    "    low,up = np.searchsorted(wl_model,[W_shifted.min(),W_shifted.max()])\n",
    "    \n",
    "    #interpolate to output - cannot provide output to np.interp function, but this would speed things up\n",
    "    out[ord] = np.interp(W_shifted,wl_model[low:up+1],template[low:up+1]) * transit_weight[:,np.newaxis] #signal relative to zero continuum\n",
    "\n",
    "    #mimic the 'blaze' correction. Add 1 to the model, divide by mean, then subtract 1 to match data\n",
    "    # (note that for large datasets even simple operations like this can be costly, so important to careful test + optimise!)\n",
    "    np.add(out[ord],1,out=out[ord])\n",
    "    np.divide(out[ord], np.mean(out[ord],axis=-1)[...,np.newaxis],out=out[ord])\n",
    "    np.subtract(out[ord], 1, out=out[ord])\n",
    "\n",
    "  return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344e670-0243-473a-8601-11211bd9d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do the same again with our new model\n",
    "logP_slice = [logP(np.array([q,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [1,1]),model3D_with_norm,*args) for q in temps]\n",
    "\n",
    "fig,ax = plt.subplots(2)\n",
    "ax[0].plot(temps,logP_slice,'r-')\n",
    "ax[1].plot(temps,np.exp(logP_slice-np.max(logP_slice)),'k-') # scale so max prob=1, no need to fully normalise\n",
    "ax[1].axvline(Temp) # plot actual value\n",
    "ax[0].set_ylabel(r'$\\ln p(T) + C$')\n",
    "ax[1].set_ylabel(r'$\\propto p(T)$')\n",
    "ax[1].set_xlabel(r'$T$')\n",
    "\n",
    "#let's define a range of temperatures and keep everything else fixed\n",
    "#alpha_slice = np.linspace(0.1,2,100) # define a range of temperatures\n",
    "#logL_slice = [log_likelihood(np.array([2300,-2,0] + [5.021643907242071] + [-5,-5] + [0,200] + [q,1]),model3D_with_norm,*args) for q in alpha_slice]\n",
    "#logP_slice = [logP(np.array([2200,-2,0] + [5.021643907242071] + [-5,-5] + [0,200] + [q,1]),model3D_with_norm,*args) for q in alpha_slice]\n",
    "logP_slice = [logP(np.array([Temp,np.log10(Pcloud),np.log10(Xray),dv,np.log10(Xh2o),np.log10(Xco)] + [v_sys,Kp] + [q,1]),model3D_with_norm,*args) for q in alpha_slice]\n",
    "\n",
    "#then lets plot them\n",
    "fig,ax = plt.subplots(2)\n",
    "#ax[0].plot(alpha_slice,logL_slice,'k-')\n",
    "ax[0].plot(alpha_slice,logP_slice,'r-')\n",
    "ax[1].plot(alpha_slice,np.exp(logP_slice-np.max(logP_slice)))\n",
    "ax[1].axvline(1) # plot actual value\n",
    "ax[0].set_ylabel(r'$\\ln p(T) + C$')\n",
    "ax[1].set_ylabel(r'$\\propto p(\\alpha)$')\n",
    "_= ax[1].set_xlabel(r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd971b-7ed8-4fc9-8bd5-8a37198101f5",
   "metadata": {},
   "source": [
    "That's already a lot better! This is just a simple example to show that mimicing *exactly* what is done to the data is the critcal way to a reliable and accurate retrieval.\n",
    "\n",
    "However, it's still not particularly great, right? The reason is that we still haven't considered how the filtering affects the model. Once we do that, we should be in much better shape.\n",
    "\n",
    "Using this new normalised setup, we could also just use our 'normal' transit model rather than the continuum subtracted version, i.e.: `args = [ph,W,R,Re,wl,model.tspec_nf,tweight]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ddca9-8947-42c8-92c4-bc8b2c92bcd4",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "The next thing we want to implement is the filter that mimics what PCA / SysRem did to our data. Here I will provide some code demonstrating how the filter works, and you can build it into the forward 3D model. If you are unfamiliar with matrix algebra, this might seems a little complicated at first, but I promise you it is very straightforward linear algebra!\n",
    "\n",
    "As discussed in the lecture, the filter is just built from the basis vectors extraced from the PCA / SysRem decomposition, as well as the (potentially important) bias term. To get the weights we just have to apply the simple linear regression functions we discussed earlier:\n",
    "$$\n",
    "\\boldsymbol {W = (U^\\top U)^{-1} U^\\top Y = U^\\dagger Y} \n",
    "$$\n",
    "To accomodate noise we can modify slightly:\n",
    "$$\n",
    "\\boldsymbol {W = (U^\\top \\boldsymbol\\Sigma^{-1} U)^{-1} U^\\top \\boldsymbol\\Sigma^{-1} Y}\n",
    "$$\n",
    "Here, $\\boldsymbol\\Sigma$ is the covariance matrix. We're going to simplify this expression by assuming a diagonal covariance matrix (i.e. with each diagonal term is $\\sigma_i^2$), and defining $\\boldsymbol{\\Sigma^{-1} = \\Lambda\\Lambda}$. This means the diagonal terms in $\\boldsymbol\\Lambda$ just contain $1/\\sigma_i$. We can now rewrite the above equation in the following way.\n",
    "$$\n",
    "\\boldsymbol {W = (U^\\top \\boldsymbol{\\Lambda\\Lambda} U)^{-1} U^\\top \\boldsymbol{\\Lambda\\Lambda} Y},\n",
    "$$\n",
    "and because $\\Lambda= \\Lambda^\\top$ we can write as:\n",
    "$$\n",
    "\\boldsymbol {W = ((\\boldsymbol{\\Lambda}U)^\\top \\boldsymbol{\\Lambda} U)^{-1} (\\boldsymbol{\\Lambda}U)^\\top \\boldsymbol{\\Lambda} Y} = (\\boldsymbol{\\Lambda}U)^\\dagger \\boldsymbol{\\Lambda Y}\n",
    "$$\n",
    "Keep in mind that $\\boldsymbol{F} = (\\boldsymbol{\\Lambda U})^\\dagger \\boldsymbol{\\Lambda}$ is just a fixed matrix, which we can store as a filtering matrix.\n",
    "\n",
    "To get the final fit to our data or model, we can just pre-multiply again by our basis vectors $\\boldsymbol{U}$:\n",
    "$$\n",
    "\\boldsymbol {M}^\\prime = \\boldsymbol{U}(\\boldsymbol{\\Lambda U})^\\dagger \\boldsymbol{\\Lambda Y} = \\boldsymbol{U F Y}.\n",
    "$$\n",
    "(note that in priniple we could incorporate the initial $\\boldsymbol {U}$ in our pre-computed filter, but due to the shapes of the matrices this is typically slower to compute - you can ask me why!)\n",
    "\n",
    "So, in order to generate a *filtered* forward model, we can simply generate the forward model as before, then apply the filter above to the 3D model, and simply subtract the result to obtain a filter model. As this is just a few matrix muliplications, this procedure is fast - much faster than PCA (where we have to determine the basis vectors as well as fit them) and orders of magnitude faster than SysRem.\n",
    "\n",
    "The following will walk through how we apply a filter to a 3D model. Remember, we already have the basis vectors $\\boldsymbol U$, and have added a bias vector to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34199922-f511-4e17-b3e6-70525a4a2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's first generate our forward model as before\n",
    "p = np.array([Temp,Pcloud,Xray,dv,Xh2o,Xco] + [v_sys,Kp]) # the model takes the 'un-logged'/original parameters\n",
    "args = [ph,W,wl,model.tspec_nf,tweight]\n",
    "\n",
    "#let's create both an unnormalised and normalised version:\n",
    "FM = model3D(p,*args)\n",
    "FMN = model3D_with_norm(p,*args)\n",
    "\n",
    "print(FM.mean())\n",
    "print(FMN.mean())\n",
    "#plot up the model\n",
    "fig,ax = plt.subplots(2)\n",
    "ax[0].pcolormesh(W[10],ph,FM[10])\n",
    "ax[1].pcolormesh(W[10],ph,FMN[10])\n",
    "ax[1].set_xlabel(r'wl $[\\AA]$')\n",
    "ax[0].set_ylabel(r'phase')\n",
    "_ = ax[1].set_ylabel(r'phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dee11e-f126-4496-9e13-8acbddd50bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compute our filter\n",
    "\n",
    "#can just get the psuedo-inverse for all orders (for >2D arrays first order is independent)\n",
    "psinv = np.linalg.pinv(U)\n",
    "\n",
    "#but in general, we want to be able to weight according to our uncertainties\n",
    "#let's then compute the mean inverse standard deviation, just the diagonal of the matrix Lambda (we never need to store a fulll diagonal matrix)\n",
    "inv_stdev = 1./np.nanmean(np.where(np.isfinite(Re),Re,np.nan),axis=-1)\n",
    "\n",
    "#then lets compute the pinv of (Lambda U)\n",
    "psinv = np.linalg.pinv(U * inv_stdev[:,:,np.newaxis])\n",
    "\n",
    "#instead however, we want to absorb the final lambda in our filter (see equations above)\n",
    "psinv_scaled = np.linalg.pinv(U * inv_stdev[:,:,np.newaxis]) * inv_stdev[:,np.newaxis,:]\n",
    "#psinv_scaled = np.linalg.pinv(U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10378154-959f-4f33-83e0-64788e9853f6",
   "metadata": {},
   "source": [
    "Ok, we are now (finally!) ready to apply our filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b05379-830c-4aed-84dd-11069d61f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's predefine memory, and just do it order by order to keep things simple\n",
    "# (you could also do it with 1 line with np.einsum, but not sure you gain much)\n",
    "filtered_model = np.empty([W.shape[0],ph.size,W.shape[1]])\n",
    "for ord in range(W.shape[0]):\n",
    "  #np.linalg.multi_dot([U[ord],psinv_scaled[ord],FMN[ord]+1],out=filtered_model[ord])\n",
    "  np.subtract(FMN[ord],np.linalg.multi_dot([U[ord],psinv_scaled[ord],FMN[ord]]),out=filtered_model[ord])\n",
    "\n",
    "#filtered_model -= 1 # subtract 1 from the filtered model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa8f56-6be6-4f6e-bd55-e996155e54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot it up and see\n",
    "\n",
    "fig,ax = plt.subplots(3,2,figsize=[10,5])\n",
    "ax[0,0].pcolormesh(W[10],ph,FM[10])\n",
    "ax[1,0].pcolormesh(W[10],ph,FMN[10])\n",
    "ax[2,0].pcolormesh(W[10],ph,filtered_model[10])\n",
    "\n",
    "for q in [30,40,50,60,70]:\n",
    "    ax[0,1].plot(W[10],FM[10,q])\n",
    "    ax[1,1].plot(W[10],FMN[10,q])\n",
    "    ax[2,1].plot(W[10],filtered_model[10,q])\n",
    "    ax[0,0].axhline(ph[q],lw=1,ls=':')\n",
    "    ax[1,0].axhline(ph[q],lw=1,ls=':')\n",
    "    ax[2,0].axhline(ph[q],lw=1,ls=':')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d71ce-8615-476b-9a82-090bb31759d2",
   "metadata": {},
   "source": [
    "And we're done pretty much done with the filtering!!\n",
    "\n",
    "One major issue that you might have noticed is that we used a single uncertainty vector / matrix for the fit. This means that we're not taking into account the uncertainties in each row. However, this isn't so bad. Recall the equation:\n",
    "$$\n",
    "\\boldsymbol {W = (U^\\top \\boldsymbol\\Sigma^{-1} U)^{-1} U^\\top \\boldsymbol\\Sigma^{-1} Y}\n",
    "$$\n",
    "If I scale my uncertainties by a constant, say by replacing $\\boldsymbol\\Sigma = \\beta\\boldsymbol\\Sigma$, and sub into the equation we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol {W} &= \\boldsymbol {W(U^\\top (\\beta\\boldsymbol\\Sigma)^{-1} U)^{-1} U^\\top (\\beta\\boldsymbol\\Sigma)^{-1} Y}\\\\\n",
    "&= \\beta\\beta^{-1}\\boldsymbol {W(U^\\top \\boldsymbol\\Sigma^{-1} U)^{-1} U^\\top \\boldsymbol\\Sigma^{-1} Y}\n",
    "\\end{align}\n",
    "$$\n",
    "i.e. the scale factor cancels. Therefore we only need to get the scaling of the noise correct. Therefore if we assume that the noise has the same time-dependence, then we can just use the average uncertainties. This is exact when we use outer-product approximations for the noise (reasonable common), but may break down in reality. Unfortunately taking the noise independently for each column (ie time-series) would require us to invert and store (or psuedo-inverse) [$N_{\\rm orders} \\times N_{\\rm \\lambda}$] matrices, which isn't super efficient. This approximation has worked for all tests I have tried, but will likely fail in certain circumstances. There are a few tricks we can use to improve this, but for now this will do!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f78c0-5b2d-4847-9be0-4a1caa137098",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Now that we have written a basic forward model, likelhood and prior, as well as looked into the filtering, it is now your turn to do some work.\n",
    "\n",
    "1) First, create a new forward (3D) model that returns a filtered model\n",
    "2) Next, re-compute the conditional distrubutions as above\n",
    "3) Pass the $\\log P$ into your favorite MCMC or inference algorithm, and see if you can run a full retrieval. You might want to try optimising the model first (though we already know the rough outcome). Getting a rough approximation of the uncertainties will always help in performing an efficient sampler (if you are new to MCMC, let me know and I can provide some code).\n",
    "4) For experienced retrievers, you could try optimising the above functions. There are speed gains to be made!\n",
    "\n",
    "For those in need of a greater challenge, I can provide reduced transit data of WASP-121b taken with UVES so you can test these methods on real data. Or you might want to try them on some of your own data!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
